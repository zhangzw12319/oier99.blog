---
title: 课程笔记：强化学习(2)——Bellman方程的动态规划求解
date: 2021-09-29 23:10:37
index_img: https://i.loli.net/2021/09/30/c6T4CF1OPVjt2Hf.jpg
banner_img: https://i.loli.net/2021/09/30/c6T4CF1OPVjt2Hf.jpg
categories:
- [课程笔记, 强化学习]
tags: [课程笔记, 强化学习]
author: oier99
comment: true
math: true
mermaid: true
excerpt: 本节只是非常粗略的把动态规划的要点总结一下，是课堂笔记纲要，并非详细讲解。先占个坑，以后有空再补
---

# 课程笔记：强化学习(2)——Bellman方程的动态规划求解



##  动态规划

{% note info %}

本节只是非常粗略的把动态规划的要点总结一下，是课堂笔记纲要，并非详细讲解。先占个坑，以后有空再补

{% endnote %}



1. 动态规划需要满足的条件：

	1. 最优子结构(Optimal substructure):

		- 原问题的最优解一定也是子问题的最优解。例如，一个输入长度为N的序列的某个Scheduling问题（记为T(1，N) ) 的最优解，拆分后一定也是其子问题T(1,N/2-1), T(N/2, N)的最优解。

		- 子问题的最优解可以通过某种形式组合和计算，得到原问题的最优解

	2. 重叠子问题

		- 子问题与原问题有同样的format和structure

	3. 无后效性

		- 计算原问题最优解时，不会对子问题最优解产生影响。

	{% note success %}

	马尔科夫链和马尔科夫决策过程满足上述三个条件的，因为:

	- Bellman方程将原问题递归的分解为规模更小的子问题
	- 子问题拥有同样的format和structure
	- Que？ 有疑问，虽然马尔科夫过程强调每个state只与上一个state有关，与其他state无关($$P(x_n|x_1,x_2,\dots,x_{n-1}) = P(x_n| x_{n-1})$$)，但这是保证MDP无后效性的原因嘛？因为通过迭代的方式求解贝尔曼方程，value早晚会传播到很多时间段以后的。

	{% endnote %}

2. Dynamic Planning in MDP:

	1. For prediction(Policy Evaluation):
		Input: MDP $$<S,A,P,R,\gamma>$$ 和策略 $ \pi $ or MRP$$<S, P^\pi, R^\pi, \gamma>$$(没有显示的策略表示，是Implicit Policy)
		Output: value function $$v_\pi$$
		对Policy Evaluation的理解：给定一个策略，通过贝尔曼方程迭代求出所有状态的价值 $$v_\pi(s)$$
	2. For control:
		Input: MDP $$<S,A,P,R,\gamma>$$
		Output: 最优价值函数$$v_*$$和最优策略$$\pi_*$$

3. Bellman方程:

  **(State-) Value Function:**

  <img src="https://i.loli.net/2021/09/29/7yIKAVjkGHTbXeU.png" alt="image-20210929174143894" style="zoom:80%;" />
$$
  \begin{align}
  v_\pi(s) &= \sum_{a}\pi(a|s)q_\pi(s,a)\\
  &= \sum_a\,\pi(a|s)(R_s^a+\gamma \sum_{s'}(P_{ss'}^a\,v_\pi(s')))(代入q_\pi(s',a)公式)
  \end{align}
$$

  求Optimal $$v_*(s)$$(把求期望操作$\rightarrow$找$\operatorname {arg\,max}$操作)：

  <img src="https://i.loli.net/2021/09/29/x9D7TQga6GShrsi.png" alt="image-20210929203919230" style="zoom:80%;" />
$$
  \begin{align}
  v_*(s) &= \underset{a}\max\{R_s^a+\gamma \sum_{s'}P_{ss'}^a\,v_\pi(s')\}
  
  \end{align}
$$


  **Action Function:**

  <img src="https://i.loli.net/2021/09/29/VMlc1N2g4DPJo5I.png" alt="image-20210929174301352" style="zoom: 80%;" />
$$
  \begin{align}
  q_*(s, a) &= R_{s}^a + \gamma\sum_{s'}\, P_{ss'}^a v_\pi(s') \\
  (or&= \sum_{s'}\, P_{ss'}^a (R_{ss'}^a + v_\pi(s')) ) (考虑奖励R_s^a是否与s'有关)\\
  &=R_s^a + \gamma\sum_{s'}\,P_{ss'}^a\sum_{a'}\pi(a'|s')q_\pi(s', a') (代入v_\pi(s')公式)\\
  \end{align}
$$
  求Optimal $$q_*(s, a)$$时(把求期望操作$\rightarrow$找$\operatorname {arg\,max}$操作)：

  <img src="https://i.loli.net/2021/09/29/xIVUrmEWBt4Nlzf.png" alt="image-20210929204248905" style="zoom:80%;" />
$$
  q_*(s, a) = R_s^a + \gamma \sum_{s'}\,P_{ss'}^a{\underset{a'}\max\,}q_\pi(s', a'))
$$



## Policy Evaluation

![image-20210929211939079](https://i.loli.net/2021/09/29/ghzERuXiC1MtsQp.png)

给定一个policy $$ \pi $$，评估/衡量在$$ \pi$$下每个状态的value function.

## Policy Iteration

![image-20210929212427612](https://i.loli.net/2021/09/29/2THt83175YAOZRJ.png)

这个是通过贪心的策略来迭代得到最好的policy。基本思想是$$ \pi(s) = \underset{a}{\operatorname{arg\,max}}\, q(s, a)$$，即在state function 固定情况下，通过贪心的选择给自己带来最大受益的action a 来作为自己的策略。可以证明经过多次迭代后，策略能收敛到贪心策略下的最优解上。

## Value Iteration

![image-20210929213203328](https://i.loli.net/2021/09/29/NpwbPFi3k52uedH.png)

这个的特点是没有给定任何显示的(explicit)策略$$ \pi $$, 而是只求解state function。

与Policy Evaluation 和Policy Iteration 不同， 前两者都出现了$$\pi(a|s)$$这个东西，也就是说每个状态下选择某个动作的概率是被记录的，并且可能随着算法也会迭代更新；而Value Iteration 每次选择受益最大的动作来更新value。

等迭代结束，value全部求出来后，最后一遍求解$$\pi$$。



## Summary

![image-20210929213757669](https://i.loli.net/2021/09/29/pNR9Q8biXWH2GOq.png)