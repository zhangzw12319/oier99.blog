---
title: è¯¾ç¨‹ç¬”è®°-ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸æ–¹æ³•-ELS-Chap7
date: 2021-11-11 00:32:57
index_img: https://i.loli.net/2021/09/30/tfhTo6NrOJKRikm.jpg
banner_img: https://i.loli.net/2021/09/30/tfhTo6NrOJKRikm.jpg
categories: 
- [è¯¾ç¨‹ç¬”è®°, ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸æ–¹æ³•]
- [æ•™æç¬”è®°, Elements of Statistical Learning]
tags: [è¯¾ç¨‹ç¬”è®°, æ•™æç¬”è®°, ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸æ–¹æ³•, ELS]
author: oier99
comment: true
math: true
mermaid: true
excerpt: æ•´ç†äº†ä¸‰éï¼Œå‰å‰ååå¤§æ¦‚ä¸€å…±èŠ±äº†20ä¸ªå°æ—¶å·¦å³å§ï¼Œè™½ç„¶ç¦»å®Œç¾è¿˜å·®å¾ˆå¤šï¼Œä½†å½¢æˆäº†ä¸€å¥—è‡ªå·±çš„ä¸œè¥¿ã€‚
---

# è¯¾ç¨‹ç¬”è®°ï¼šç»Ÿè®¡å­¦ä¹ ç†è®ºä¸æ–¹æ³•ï¼ˆELS_Chap7ï¼‰

ğŸ“šä¹¦ç±ï¼šã€ŠElements of Statistical Learningã€‹Chap 7



## ä¸¤ä¸ªæ¦‚å¿µ

1. æ¨¡å‹é€‰æ‹©(Model Selection):  æ¨¡å‹ç”±å‚æ•°æ§åˆ¶çš„ï¼Œæ¨¡å‹é€‰æ‹©æ—¢åŒ…æ‹¬æ¨¡å‹ç±»å‹é€‰æ‹©ï¼Œä¹ŸåŒ…æ‹¬å‚æ•°çš„æ§åˆ¶ã€‚åé¢è®²è´å¶æ–¯ä¿¡æ¯å‡†åˆ™æ—¶(BIC)ä¼šè®²åˆ°ã€‚
2. æ¨¡å‹è¯„ä¼°(Model Assessment):  ç»™å®šä¸€ä¸ªæ¨¡å‹ï¼Œè¯„ä¼°å…¶è®­ç»ƒè¯¯å·®(In-sample Error)ï¼Œæµ‹è¯•è¯¯å·®(Extra-sample Error)ï¼Œæ³›åŒ–è¯¯å·®(Generalization Error)ã€‚è¯„ä¼°è®­ç»ƒè¯¯å·®ä¸æµ‹è¯•è¯¯å·®ä¹‹é—´çš„å·®è·Bound.

ä¸¤è€…éƒ½æ˜¯ä¾§é‡å¯¹æ¨¡å‹çš„æ³›åŒ–è¯¯å·®è¿›è¡Œè¯„ä¼°ï¼Œè€Œéæ¨¡å‹çš„è®­ç»ƒè¯¯å·®ã€‚æ³›åŒ–è¯¯å·®æ˜¯æ•´ä¸ªç»Ÿè®¡å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„å…³æ³¨é—®é¢˜ã€‚


## è®­ç»ƒè¯¯å·®ä¸æ³›åŒ–è¯¯å·®

è®­ç»ƒè¯¯å·®ï¼š 
$$
\overline{\text{err}} = \frac{1}{N} \sum_{i=1}^N L(y_i, \hat{f}(x_i))
$$
æ³›åŒ–è¯¯å·®ï¼š
$$
\text{Err}(y_i, \hat{f}(x_i)) = E_{(X,Y) \sim \tau}(L(y_i, \hat{f}(x_i)) |\tau)
$$
å…¶ä¸­æŸå¤±å‡½æ•°é€šå¸¸æ˜¯:

- å‡æ–¹è¯¯å·®$$L = (y_i - \hat{f}(x_i))^2 $$
- å¯¹æ•°æå¤§ä¼¼ç„¶å‡½æ•°ã€ç”¨äºå¤šåˆ†ç±»ã€‘(ä¸¥æ ¼æ¥è¯´è¿™ä¸ç®—æŸå¤±å‡½æ•°ï¼Œä½†æ˜¯æå¤§ä¼¼ç„¶æœ€å¤§ç­‰ä»·äºåŠ ç¬¦å·æœ€å°ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆç›¸åŒå½¢å¼):$$L = -2\cdot[\sum_{k=1}^K I(G=k)\log(\hat{\Pr}(\hat{G}=k)] $$.å…¶ä¸­$$I(G=k)$$ç›¸å½“äºçœŸå®æ ‡ç­¾(Ground Truth).
- å¯¹æ•°æå¤§ä¼¼ç„¶çš„ç‰¹æ®Šæƒ…å†µã€0-1åˆ†ç±»ã€‘ï¼š $$L = I(G=1)\log(\hat{\Pr}) + (1-I(G=1))\log(1-\hat{\Pr})$$

{% note primary%}

ğŸ“¢æ³¨æ„ï¼Œæˆ‘ä»¬ç®—è¯¯å·®æ—¶çš„Ground Truth(GT) $$y$$å¹¶ä¸ä¸€å®šæ˜¯çœŸå®çš„æ ‡ç­¾ï¼Œå› ä¸ºè·å–æ•°æ®çš„è¿‡ç¨‹ä¸­æ— æ³•é¿å…æœ‰å™ªå£°å­˜åœ¨(å°±æ¯”å¦‚ç”¨ç²¾å¯†ç‰©ç†ä»ªå™¨æµ‹é‡çš„ç»“æœæ€»ä¼šæœ‰æ— æ³•é¿å…çš„ç³»ç»Ÿè¯¯å·®)ã€‚æˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ˜¯è®©æ¨¡å‹çš„ä¼°è®¡å€¼$$\hat{f}$$å’ŒGTç®—è¯¯å·®ï¼Œä¸æ˜¯å’Œ$$f(x)$$ç®—è¯¯å·®ã€‚

{% endnote %}

## Bias-variance åˆ†è§£

å‡è®¾ï¼š$$\{(x_i,y_i)\}$$ æ˜¯ä»æŸä¸ªæ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„æŸä¸ªæ•°æ®é›†$$\tau$$ã€‚æ•°æ®çš„çœŸå®åˆ†å¸ƒæ˜¯$$y = f(x) + \epsilon$$ï¼Œæ•°æ®çš„çœŸå®æ ‡ç­¾æ˜¯$$f(x)$$ï¼Œä½†æ˜¯æˆ‘ä»¬è·å–æ•°æ®é›†æ—¶æ€»ä¼šæœ‰å™ªå£°$$\epsilon$$å¹²æ‰°ã€‚é€šå¸¸å‡è®¾å™ªå£°æœä»å‡å€¼ä¸º0çš„æŸä¸ªé«˜æ–¯åˆ†å¸ƒã€‚$$\epsilon \sim N(0, \sigma_{\epsilon}^2)$$



è€ƒå¯Ÿåœ¨$$X=x_0$$ä¸€ç‚¹ä¸Šçš„å•ç‚¹æ³›åŒ–è¯¯å·®ï¼Œå…¶ä¸­æ±‚æœŸæœ›æ“ä½œ$$E$$æ˜¯å¯¹æ‰€æœ‰çš„æ•°æ®é›†$$\tau$$æ±‚çš„æœŸæœ›(Expectation over all the dataset $$\tau$$).<a name="t1">ä¹Ÿå°±æ˜¯è¯´åœ¨è¿™é‡Œï¼Œ$\hat{f}(x_0)$ä¹Ÿæ˜¯éšæœºå˜é‡ï¼Œå®ƒçš„éšæœºæ€§æ¥æºäºå¯èƒ½é€‰å–ä¸åŒçš„æ•°æ®é›†$\tau$ ï¼›è€Œ$f(x_0)$çš„éšæœºæ€§åªæ¥æºäº$\epsilon$ï¼ŒäºŒè€…ç‹¬ç«‹æ— å…³è”</a>ã€‚
$$
\begin{align*}
E(L(Y, \hat{f}(X))|X=x_0, Y=y_0 )&= (y_0 - f(x_0)) \,\,\,\text{(è¿™é‡Œé‡‡ç”¨å‡æ–¹è¯¯å·®åˆ†æï¼Œå…¶ä»–åŒç†)}\\
&= E(f(x_0) - \hat{f}(x_0) + \epsilon)^2 \\
&= E(f(x_0) - \hat{f}(x_0))^2 + E(\epsilon)^2 + 2E(f(x_0) - \hat{f}(x_0))E(\epsilon)\,\,\,\text{($\epsilon$ ä¸ $f(x_0) - \hat{f(x_0)}$ ç‹¬ç«‹)} \\
&=  E(f(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 + 0 \\
&= E(f(x_0) - E\hat{f}(x_0) + E\hat{f}(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 \\
&= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2 + 2E(f(x_0) - E\hat{f}(x_0))\cdot 0 + \sigma_{\epsilon}^2\\
&= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2  + \sigma_{\epsilon}^2 \\
&= \text{bias}^2 + \text{variance} + \sigma_{\epsilon}^2
\end{align*}
$$



{% note info %}

å…³äºæœŸæœ›ä¸æ–¹å·®çš„å¸¸ç”¨æ€§è´¨ï¼Œè¯·å‚è§è¿™ç¯‡[åšå®¢](https://www.oier99.cn/2021/11/10/éšç¬”-æœŸæœ›ä¸æ–¹å·®çš„æ€§è´¨/).

{% endnote%}



### KNNçš„ä¾‹å­

KNNçš„æ¨¡å‹æ˜¯$$\hat{f}(x_0) = \frac{1}{K}\sum_{i=1}^K y_i,\, x_i \in n_K(x_0)$$.
$$
\begin{align*}
\hat{f}(x_0) &= \frac{1}{K}\sum_{i=1}^K y_i \\
&= \frac{1}{K} \sum_{i=1}^K(f(x_i) + \epsilon_i ) \\
&\approx \frac{1}{K} \sum_{i=1}^K(f(x_0) + \epsilon_i ) \\
&= f(x_0) + \frac{1}{K} \sum_{i=1}^K \epsilon_i
\end{align*}
$$
ä¸ºäº†è®¡ç®—æ–¹å·®æ–¹ä¾¿ï¼Œ åœ¨ç†è®ºæ¨å¯¼è¿™é‡Œåšäº†ä¸€ä¸ªé‡è¦è¿‘ä¼¼ï¼šå› ä¸ºKNNå–çš„æ˜¯Kä¸ªè¿‘é‚»ç‚¹æ¥æ’å€¼ä¼°è®¡ï¼Œæ‰€ä»¥å‡è®¾è®¤ä¸ºä»–ä»¬çš„â€œæœ¬æºâ€$$f(x_i) \approx f(x_0)$$.è¿™æ ·ï¼ŒKNNçš„æ–¹å·®å¯ä»¥å¦‚ä¸‹è®¡ç®—:
$$
\begin{align*}
\text{Var}(\hat{f}(x_0)) &= \text{Var} \frac{1}{K} \sum_{i=1}^K \epsilon_i \\
&= \frac{1}{K^2} \text{Var}\sum_{i=1}^K \epsilon_i \\
&= \frac{1}{K^2} \sum_{i=1}^K \text{Var}(\epsilon_i) \\
&= \frac{1}{K^2} \sum_{i=1}^K \sigma_{\epsilon}^2 \\
&= \frac{1}{K} \sigma_{\epsilon}^2
\end{align*}
$$
æ•…æ³›åŒ–è¯¯å·®æ‹†åˆ†ä¸º:
$$
Err(y_0, \hat{f}(x_0)) \approx \sigma_{\epsilon}^2 + E(f(x_0) - \frac{1}{K}\sum_{i=1}^K y_i )^2 + \frac{1}{K} \sigma_{\epsilon}^2
$$
å¯ä»¥çœ‹å‡º$$K$$è¶Šå°ï¼Œæ¨¡å‹å¤æ‚åº¦è¶Šå¤§ï¼Œæ–¹å·®è¶Šå¤§ï¼Œbiasåº”è¯¥ä¼šè¶Šå°ã€‚

### çº¿æ€§å›å½’çš„ä¾‹å­

å¯¹äºçº¿æ€§å›å½’å‡½æ•°$$\hat{f}(x) = \hat{\beta}^T x $$, è§£æœ€å°äºŒä¹˜è¯¯å·®ä¸‹çš„æœ€ä½³è¿‘ä¼¼å‚æ•°æ˜¯$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

æ•…$$\hat{f}(x) = y^T X(X^TX)^{-1}x = x^T X(X^TX)^-1X^Ty = h(x)y$$.å¯ä»¥çœ‹å‡º$$\hat{f}(x)$$çš„éšæœºæ€§æ¥æºäºyï¼Œå³æ¥æºäº$$\epsilon$$ .ï¼ˆè¿™ä¸[åˆšæ‰](#t1)è¯´æ³•ä¸çŸ›ç›¾ã€‚å› ä¸ºä¹‹å‰å¯¹$$\hat{f}$$çš„åˆ†ææ˜¯æŠ½è±¡çš„ç¬¦å·ï¼Œè€Œè¿™é‡Œæ˜¯å¯¹çº¿æ€§å›å½’å…·ä½“å…¬å¼åˆ†æï¼‰
$$
\text{var}(\hat{f}(x_0)) = ||h(x_0)||^2 \sigma_{\epsilon}^2
$$
æ•…
$$
Err(y_0, \hat{f}(x_0)) = \sigma_{\epsilon}^2 + E(f(x_0) - \hat{\beta}^Tx_0 )^2 + ||h(x_0)||^2 \sigma_{\epsilon}^2
$$


è®­ç»ƒè¯¯å·®:
$$
\frac{1}{N}\sum_{i=1}^N Err(y_0, \hat{f}(x_0)) = \sigma_{\epsilon}^2 + \sum_{i=1}^NE(f(x_0) - \hat{\beta}^Tx_0 )^2 + \frac{p}{N}\sigma_{\epsilon}^2
$$
å¯ä»¥çœ‹å‡ºæ¨¡å‹å¤æ‚åº¦ç”±æ•°æ®é‡$$N$$å’Œå‚æ•°é‡$$p$$å…±åŒæ§åˆ¶ã€‚

{% note primary %}

ï¼ˆè¿™éƒ¨åˆ†çš„æ¨å¯¼éœ€è¦ç”¨åˆ°çŸ©é˜µçš„è¿¹çš„æ€§è´¨ï¼‰

$$\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$$

$$\sum_{i}(x_i y_i) = x^Ty = \text{tr} (xy^T)$$

{%endnote%}



## ä¹è§‚åº¦

ä¹è§‚åº¦(Optimisim)çš„æ¦‚å¿µä¸»è¦æ˜¯ä¸ºäº†æ¯”è¾ƒæ¨¡å‹è®­ç»ƒè¯¯å·®ä¸æ³›åŒ–è¯¯å·®ä¹‹é—´çš„å·®è·ï¼Œæˆ–è€…å¦‚ä½•ç”¨è®­ç»ƒè¯¯å·®å»ä¼°è®¡æ³›åŒ–è¯¯å·®ã€‚

ç›´æ¥è®¡ç®—è®­ç»ƒè¯¯å·®å’Œæ³›åŒ–è¯¯å·®çš„å·®æœ‰å›°éš¾ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥$$x$$éƒ½ä¸å›ºå®šã€‚æ‰€ä»¥å¯ä»¥é‡‡ç”¨é‡é‡‡æ ·çš„æŠ€æœ¯ï¼ŒåŸå…ˆè®­ç»ƒé›†$$(x,y)$$,æ˜¯è®­ç»ƒæ—¶é‡‡æ ·å¾—åˆ°çš„æ ‡ç­¾ï¼Œè®°ä¸º$$y$$ï¼›é‡é‡‡æ ·æ˜¯å¯¹ç›¸åŒçš„$$x$$, å†æ¬¡é‡‡æ ·$$y^{\text{New}}$$ã€‚
$$
\text{Err}_{in}(y_i, \hat{f}(x_i)) = \frac{1}{N}\sum_{i=1}^N E_{Y^{\text{new}}}[L(Y_i^{\text{New}}, \hat{f}(x_i))]
$$

$$
\overline{err} = \frac{1}{N} \sum_{i=1}^N L(y, \hat{f}(x_i))
$$



ç»“è®º
$$
\begin{align*}
	w &= E_y(op) \\
	&= \frac{2}{N}\text{Cov}(y, \hat{y})
\end{align*}
$$
è¯æ˜è¿‡ç¨‹åç»­å†è¡¥ã€‚



è‹¥$$\hat{y}$$æ˜¯ç”±å‚æ•°é‡ä¸º$$d$$çº¿æ€§å›å½’å¾—åˆ°çš„ä¼°è®¡ï¼Œé‚£ä¹ˆ
$$
\text{Cov}(y, \hat{y}) = d \sigma_{\epsilon}^2
$$
å¯å¾—åˆ°
$$
E_y(\text{Err}_{in}) = E_y(\overline{\text{err}}) + 2\cdot\frac{d}{N} \sigma_{\epsilon}^2
$$


## æœ‰æ•ˆå‚æ•°é‡

$$
\hat{y} = Sy
$$

æœ‰æ•ˆå‚æ•°æ•°é‡(Effective Number of Parameters):
$$
\text{df}(S) = \text{tr}(S)
$$
å¯¹äºçº¿æ€§å›å½’ï¼Œæœ‰å¦‚ä¸‹å…³ç³»ï¼š
$$
\sum_{i=1} ^N \text{Cov}(y_i, \hat{y}_i) = \text{df}(\hat{y}) \cdot  \sigma_{\epsilon}^2
$$

## è´å¶æ–¯ä¿¡æ¯é‡ï¼ˆBICï¼‰

é¢˜å¤–è¯ï¼šå‰é¢çš„éƒ¨åˆ†ä¸»è¦è®²è¿°çš„æ¨¡å‹è¯„ä»·(Model Assesment)éƒ¨åˆ†ï¼Œè´å¶æ–¯ä¿¡æ¯é‡ä¸»è¦è®²è¿°çš„æ˜¯æ¨¡å‹é€‰æ‹©çš„éƒ¨åˆ†

BIC for æå¤§ä¼¼ç„¶å›å½’
$$
BIC = -2 \cdot loglik + (\log N) \cdot d
$$
BIC under Gaussian Model
$$
BIC = \frac{N}{\sigma_{\epsilon}^2}[\overline{\text{err}}+ (\log N)\cdot \frac{d}{N} \sigma_{\epsilon}^2]
$$
ä»–çš„Motivationæ˜¯æºè‡ªè´å¶æ–¯æ´¾çš„æ€æƒ³(ä¸‹é¢æ‰‹æŠ„è‰ç¨¿ï¼Œç”±äºè¿‡ç¨‹æœ‰ç‚¹éš¾ï¼Œæ•´ç†åè¿˜æœ‰å¾ˆå¤šé”™è¯¯æ¯”å¦‚ç¬¦å·ä¸Šä¸‹æ ‡å¯¹ä¸ä¸Š)

![1](https://i.loli.net/2021/11/10/9qVUZlvD2MgstSo.jpg)

![2](https://i.loli.net/2021/11/10/yYAO547q1Nf39cM.jpg)

![3](https://i.loli.net/2021/11/10/uskaSvyOilJT8mU.jpg)



## äº¤å‰éªŒè¯(ä»¥åæœ‰ç©ºå†è¡¥)

